# ğŸ“˜ IOTA vs Blockchain â€“ Benchmark Suite (Simulated)

This project implements a **complete reproducible benchmark suite** to compare the performance of **IOTA** and **Ethereum/Sepolia** in a **fully simulated environment**, eliminating network noise and enabling rigorous statistical analysis.

It includes:
- **Latency**, **throughput**, and **scalability** tests  
- **Deterministic simulation** using seeded RNG  
- **Replicated executions** (N=30 by default)  
- Automatic computation of:
  - Mean  
  - Variance  
  - Standard deviation  
  - **95% Confidence Intervals (Normal distribution)**  
- Export to **CSV**  
- Full **Python analysis + plots**

---

## ğŸ§ª 1. Included Tests

### **1. Latency Test (Simulated)**
Measures:
- Average latency  
- p95 latency  
- CPU time  
- Success rate  

For networks:
- **Ethereum (Sepolia) â€“ simulated**
- **IOTA â€“ simulated**

---

### **2. Throughput Test (Simulated)**
Runs a batch of parallel simulated transactions/blocks.

Metrics:
- TPS (transactions per second)  
- Average latency  
- Success rate  

---

### **3. Scalability Test (Simulated)**
Evaluates performance for increasing batch sizes:

```
10, 100, 1000, 10000
```

For each batch:
- TPS  
- Latency  
- Success rate  

---

## ğŸ” 2. Replicated Executions (Monte Carlo)

`run-simulated-benchmarks.js` includes:

- `withSeed(seed, fn)`  
- `runReplicatedSuite(N)`  

This runs all benchmark tests **multiple times**, each with a fixed seed, enabling:

- Perfect reproducibility  
- Noise reduction  
- Statistical confidence  

## ğŸ“ 95% Confidence Interval (Normal Distribution)

The 95% confidence interval is computed as:

$$
CI_{95} = \bar{x} \pm 1.96 \cdot \frac{s}{\sqrt{n}}
$$

Where:

- **$\bar{x}$** = sample mean  
- **$s$** = sample standard deviation  
- **$n$** = number of replications  

Exported metrics include:

- `mean`  
- `variance`  
- `std`  
- `ci95_lower`  
- `ci95_upper`  
- `n`  

---

## ğŸ“ 3. Project Structure

```
src/
  ethereum/
  iota/
  tests/
    run-simulated-benchmarks.js
results/
  *.csv       â† auto-generated benchmark data
  *.png       â† graphs created by the analysis script
logs/
analysis/
  analyze_benchmarks.py
```

---

## â–¶ï¸ 4. Running the Benchmarks

### **Run the default simulated suite**
```bash
node src/tests/run-simulated-benchmarks.js
```

### **Run only the replicated suite**
Inside the file:
```js
runReplicatedSuite(30)
```

Or via CLI (if enabled):
```bash
node src/tests/run-simulated-benchmarks.js --replications 30
```

---

## ğŸ“Š 5. Analysis & Visualization (Python)

Use the analysis script:

```
analysis/analyze_benchmarks.py
```

Run:
```bash
python analyze_benchmarks.py
```

This generates:

### **Markdown summary (SUMMARY.md)**
- Latency (mean + CI95)  
- p95 latency (CI95)  
- Throughput (CI95)  
- Scalability per batch size (CI95)  
- Success rate CI95  

### **Graphs (PNG)**
- `latency_ci95.png`
- `latency_p95_ci95.png`
- `latency_success_ci95.png`
- `throughput_ci95.png`
- `throughput_success_ci95.png`
- `scalability_tps_ci95.png`
- `scalability_latency_ci95.png`
- `scalability_success_ci95.png`

---

## ğŸ“¦ 6. How to Interpret the Results

This suite allows answering:

âœ” Which network is faster?  
âœ” Which network achieves higher throughput?  
âœ” How does performance change with load?  
âœ” How stable are results across seeds?  
âœ” Are differences **statistically significant**?

### **Significance rule:**

- **If CI95 intervals do NOT overlap â†’ significant difference**
- **If they overlap â†’ not statistically significant**

---

## â™»ï¸ 7. Cleaning Up Results

Not required, but you may wipe previous results:

```bash
rm -rf results/* logs/*
```

---

## ğŸ§± 8. Requirements

- **Node.js â‰¥ 18**  
- **Python â‰¥ 3.10**

Python libraries:
```bash
pip install pandas matplotlib
```

---

## ğŸ›  9. Optional Enhancements

I can add:
- Full CLI flags (`--replicas`, `--tag`, `--skip-tests`)
- Parquet or JSONL export  
- Automatic PDF report  
- Jupyter Notebook  
- Real-network benchmark mode  
