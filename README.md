Here is the full English version of the README ‚Äî clear, professional, and ready to use:

‚∏ª

üìò IOTA vs Blockchain ‚Äì Benchmark Suite (Simulated)

This project implements a complete reproducible benchmark suite to compare the performance of IOTA and Ethereum/Sepolia under a fully simulated environment, eliminating network noise and enabling rigorous statistical analysis.

It includes:
	‚Ä¢	Latency, throughput, and scalability tests
	‚Ä¢	Deterministic simulation using seeded RNG
	‚Ä¢	Replicated executions (N=30 by default)
	‚Ä¢	Automatic computation of:
	‚Ä¢	Mean
	‚Ä¢	Variance
	‚Ä¢	Standard deviation
	‚Ä¢	95% Confidence Intervals (Normal distribution)
	‚Ä¢	Automatic CSV export
	‚Ä¢	Full Python analysis with visualizations

‚∏ª

üß™ 1. Included Tests

1. Latency Test (Simulated)

Measures:
	‚Ä¢	Average latency
	‚Ä¢	p95 latency
	‚Ä¢	CPU time
	‚Ä¢	Success rate

For networks:
	‚Ä¢	Ethereum (Sepolia ‚Üí simulated)
	‚Ä¢	IOTA (submitBlock ‚Üí simulated)

‚∏ª

2. Throughput Test (Simulated)

Sends a parallel batch of simulated transactions/blocks.

Collects:
	‚Ä¢	Throughput (TPS)
	‚Ä¢	Average latency
	‚Ä¢	Success rate

‚∏ª

3. Scalability Test (Simulated)

Evaluates performance across multiple batch sizes:

10, 100, 1000, 10000

For each size:
	‚Ä¢	TPS
	‚Ä¢	Average latency
	‚Ä¢	Success rate

‚∏ª

üîÅ 2. Replicated Executions (Monte Carlo)

The file run-simulated-benchmarks.js includes:
	‚Ä¢	withSeed(seed, fn)
	‚Ä¢	runReplicatedSuite(N)

This allows running each test N times, each with a fixed RNG seed, giving:
	‚Ä¢	Full reproducibility
	‚Ä¢	Noise reduction
	‚Ä¢	Statistically robust metrics

Statistics computed per metric

Using normal distribution, as requested:

CI95 = \bar{x} \pm 1.96 \cdot \frac{s}{\sqrt{n}}

The system computes and exports:
	‚Ä¢	mean
	‚Ä¢	variance
	‚Ä¢	std
	‚Ä¢	ci95_lower
	‚Ä¢	ci95_upper
	‚Ä¢	n (number of replications)

‚∏ª

üìÅ 3. Project Structure

src/
  ethereum/
  iota/
  tests/
    run-simulated-benchmarks.js
results/
  *.csv      ‚Üê auto-generated
  *.png      ‚Üê generated by analysis script
logs/
analysis/
  analyze_benchmarks.py


‚∏ª

‚ñ∂Ô∏è 4. Running the Benchmarks

Run the standard benchmark suite:

node src/tests/run-simulated-benchmarks.js

Run only the replicated suite:

runReplicatedSuite(30)

Or from CLI (if CLI mode added):

node src/tests/run-simulated-benchmarks.js --replications 30


‚∏ª

üìä 5. Analysis & Visualization (Python)

Use the script:

analysis/analyze_benchmarks.py

Run:

python analyze_benchmarks.py

This generates:

Tables (in SUMMARY.md)
	‚Ä¢	Latency (mean + CI95)
	‚Ä¢	p95 Latency (CI95)
	‚Ä¢	Throughput (mean + CI95)
	‚Ä¢	Scalability metrics per batch size (CI95)
	‚Ä¢	Success rate CI95

Graphs (PNG)
	‚Ä¢	latency_ci95.png
	‚Ä¢	latency_p95_ci95.png
	‚Ä¢	latency_success_ci95.png
	‚Ä¢	throughput_ci95.png
	‚Ä¢	throughput_success_ci95.png
	‚Ä¢	scalability_tps_ci95.png
	‚Ä¢	scalability_latency_ci95.png
	‚Ä¢	scalability_success_ci95.png

‚∏ª

üì¶ 6. How to Interpret the Results

This suite allows you to answer:

‚úî Which network is faster?
‚úî Which network provides higher throughput?
‚úî How do networks behave under increasing load?
‚úî How stable are the results across seeds?
‚úî Are the differences statistically significant?

Statistical interpretation:
	‚Ä¢	If CI95 intervals do NOT overlap ‚Üí
Significant performance difference.
	‚Ä¢	If CI95 intervals overlap ‚Üí
Differences may be due to simulation noise.

‚∏ª

‚ôªÔ∏è 7. Cleaning Up Results

It‚Äôs not required to delete previous logs/CSVs, since files have timestamps.

To wipe everything clean:

rm -rf results/* logs/*


‚∏ª

üß± 8. Requirements
	‚Ä¢	Node.js ‚â• 18
	‚Ä¢	Python ‚â• 3.10

Python dependencies:

pip install pandas matplotlib


‚∏ª

üõ† 9. Recommended Next Enhancements

If you want, I can add:
	‚Ä¢	Full CLI flags (--replicas, --tag, --skip-tests)
	‚Ä¢	Outputs to Parquet or JSONL
	‚Ä¢	Auto-generated PDF report
	‚Ä¢	Jupyter-ready notebook
	‚Ä¢	Real-network benchmarking mode
